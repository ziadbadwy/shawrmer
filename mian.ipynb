{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziadz\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tracker import*\n",
    "import cvzone\n",
    "model=YOLO('yolov8m.pt')\n",
    "\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE :  \n",
    "        point = [x, y]\n",
    "        print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"coco.txt\", \"r\")\n",
    "data = my_file.read()\n",
    "class_list = data.split(\"\\n\") \n",
    "#print(class_list)\n",
    "\n",
    "count=0\n",
    "persondown={}\n",
    "tracker=Tracker()\n",
    "counter1=[]\n",
    "\n",
    "personup={}\n",
    "counter2=[]\n",
    "cy1=194\n",
    "cy2=220\n",
    "offset=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 24 persons, 301.0ms\n",
      "Speed: 5.0ms preprocess, 301.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[513, 444]\n",
      "\n",
      "0: 320x640 26 persons, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 backpack, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 2 backpacks, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 1 handbag, 1 tennis racket, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 3 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 30 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 24 persons, 2 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 1 handbag, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 2 backpacks, 1 handbag, 1 suitcase, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 1 suitcase, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 handbag, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 2 handbags, 1 suitcase, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[508, 443]\n",
      "\n",
      "0: 320x640 28 persons, 1 handbag, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[505, 442]\n",
      "[503, 441]\n",
      "\n",
      "0: 320x640 29 persons, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[497, 438]\n",
      "\n",
      "0: 320x640 27 persons, 2 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[489, 434]\n",
      "[485, 433]\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[477, 430]\n",
      "[476, 430]\n",
      "\n",
      "0: 320x640 25 persons, 1 handbag, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[467, 428]\n",
      "\n",
      "0: 320x640 25 persons, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[462, 426]\n",
      "[461, 426]\n",
      "\n",
      "0: 320x640 23 persons, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[457, 425]\n",
      "[456, 425]\n",
      "\n",
      "0: 320x640 24 persons, 2 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[450, 422]\n",
      "[450, 421]\n",
      "\n",
      "0: 320x640 24 persons, 3 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[437, 413]\n",
      "\n",
      "0: 320x640 25 persons, 3 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[422, 401]\n",
      "[419, 398]\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[405, 387]\n",
      "[404, 387]\n",
      "\n",
      "0: 320x640 25 persons, 1 handbag, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[390, 374]\n",
      "[384, 365]\n",
      "\n",
      "0: 320x640 24 persons, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[374, 344]\n",
      "[374, 343]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[366, 316]\n",
      "[365, 315]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[357, 290]\n",
      "[357, 289]\n",
      "\n",
      "0: 320x640 25 persons, 4 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[350, 270]\n",
      "[350, 268]\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[347, 253]\n",
      "[347, 251]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[347, 239]\n",
      "[347, 238]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[353, 226]\n",
      "[360, 220]\n",
      "\n",
      "0: 320x640 25 persons, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[371, 212]\n",
      "[372, 212]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[384, 206]\n",
      "\n",
      "0: 320x640 26 persons, 3 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[391, 202]\n",
      "[392, 202]\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[394, 200]\n",
      "[395, 200]\n",
      "\n",
      "0: 320x640 28 persons, 5 handbags, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 4 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[397, 199]\n",
      "[398, 198]\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[400, 198]\n",
      "[401, 198]\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[402, 198]\n",
      "\n",
      "0: 320x640 29 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[402, 199]\n",
      "\n",
      "0: 320x640 27 persons, 1 car, 1 handbag, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[402, 200]\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[402, 201]\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 24 persons, 1 backpack, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 4 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 backpack, 2 handbags, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[402, 204]\n",
      "[403, 204]\n",
      "\n",
      "0: 320x640 24 persons, 1 backpack, 3 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[410, 212]\n",
      "[422, 215]\n",
      "\n",
      "0: 320x640 25 persons, 1 backpack, 4 handbags, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[456, 215]\n",
      "[460, 214]\n",
      "\n",
      "0: 320x640 23 persons, 1 car, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[510, 203]\n",
      "[513, 202]\n",
      "\n",
      "0: 320x640 25 persons, 1 car, 1 handbag, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[570, 188]\n",
      "[574, 187]\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 1 handbag, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[632, 171]\n",
      "[638, 169]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[702, 141]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[766, 109]\n",
      "[770, 106]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[832, 71]\n",
      "\n",
      "0: 320x640 25 persons, 2 handbags, 1 suitcase, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[877, 41]\n",
      "[882, 38]\n",
      "\n",
      "0: 320x640 28 persons, 3 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[909, 19]\n",
      "[910, 18]\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[922, 6]\n",
      "\n",
      "0: 320x640 26 persons, 3 handbags, 1 tennis racket, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 2 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 4 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 4 handbags, 1 tennis racket, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 3 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 26 persons, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 25 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 backpack, 1 handbag, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 30 persons, 1 handbag, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 2 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 30 persons, 2 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 2 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 28 persons, 2 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 29 persons, 2 handbags, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 27 persons, 1 backpack, 2 handbags, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 30 persons, 2 backpacks, 2 handbags, 1 suitcase, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[976, 1]\n",
      "\n",
      "0: 320x640 27 persons, 1 backpack, 1 handbag, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[951, 21]\n",
      "[941, 28]\n",
      "\n",
      "0: 320x640 28 persons, 1 backpack, 2 handbags, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[902, 53]\n",
      "[887, 63]\n",
      "[884, 65]\n",
      "\n",
      "0: 320x640 26 persons, 1 backpack, 1 handbag, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[847, 81]\n",
      "[846, 82]\n",
      "\n",
      "0: 320x640 28 persons, 1 handbag, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[825, 85]\n",
      "[820, 84]\n",
      "[819, 83]\n",
      "\n",
      "0: 320x640 25 persons, 2 handbags, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[803, 73]\n",
      "[802, 72]\n",
      "\n",
      "0: 320x640 26 persons, 2 handbags, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[788, 56]\n",
      "[788, 55]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "cap=cv2.VideoCapture('vidp/vidp.mp4')\n",
    "\n",
    "while True:    \n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "#    frame = stream.read()\n",
    "    original_video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    count += 1\n",
    "    if count % 3 != 0:\n",
    "        continue\n",
    "    frame=cv2.resize(frame,(1020,500))\n",
    "   \n",
    "\n",
    "    results=model.predict(frame)\n",
    " #   print(results)\n",
    "    a=results[0].boxes.data.cpu().numpy()\n",
    "    px=pd.DataFrame(a).astype(\"float\")\n",
    "#    print(px)\n",
    "    list=[]\n",
    "   \n",
    "    for index,row in px.iterrows():\n",
    "#        print(row)\n",
    " \n",
    "        x1=int(row[0])\n",
    "        y1=int(row[1])\n",
    "        x2=int(row[2])\n",
    "        y2=int(row[3])\n",
    "        d=int(row[5]) # class id\n",
    "        \n",
    "        c=class_list[d]\n",
    "        if 'person' in c:\n",
    "\n",
    "            list.append([x1,y1,x2,y2])\n",
    "       \n",
    "        \n",
    "    bbox_id=tracker.update(list)\n",
    "    for bbox in bbox_id:\n",
    "        x1,y1,x2,y2,id=bbox\n",
    "        cx=int(x1+x2)//2\n",
    "        cy=int(y1+y2)//2\n",
    "        cv2.circle(frame,(cx,cy),4,(255,0,255),-1)\n",
    "        if cy1<(cy+offset) and cy1>(cy-offset):\n",
    "            cv2.rectangle(frame,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            cvzone.putTextRect(frame,f'{id}',(x1,y1),1,2)\n",
    "            persondown[id]=(cx,cy)\n",
    "\n",
    "    cvzone.putTextRect(frame,f'person : {len(persondown)}',(50,60),1,2)\n",
    "    cv2.line(frame,(3,194),(1018,192),(0,255,0),2)\n",
    "    cv2.line(frame,(5,220),(1019,220),(0,255,255),2)\n",
    "\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1)&0xFF==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A class for storing and manipulating detection boxes.\n",
      "\n",
      "    Args:\n",
      "        boxes (torch.Tensor | numpy.ndarray): A tensor or numpy array containing the detection boxes,\n",
      "            with shape (num_boxes, 6) or (num_boxes, 7). The last two columns contain confidence and class values.\n",
      "            If present, the third last column contains track IDs.\n",
      "        orig_shape (tuple): Original image size, in the format (height, width).\n",
      "\n",
      "    Attributes:\n",
      "        xyxy (torch.Tensor | numpy.ndarray): The boxes in xyxy format.\n",
      "        conf (torch.Tensor | numpy.ndarray): The confidence values of the boxes.\n",
      "        cls (torch.Tensor | numpy.ndarray): The class values of the boxes.\n",
      "        id (torch.Tensor | numpy.ndarray): The track IDs of the boxes (if available).\n",
      "        xywh (torch.Tensor | numpy.ndarray): The boxes in xywh format.\n",
      "        xyxyn (torch.Tensor | numpy.ndarray): The boxes in xyxy format normalized by original image size.\n",
      "        xywhn (torch.Tensor | numpy.ndarray): The boxes in xywh format normalized by original image size.\n",
      "        data (torch.Tensor): The raw bboxes tensor (alias for `boxes`).\n",
      "\n",
      "    Methods:\n",
      "        cpu(): Move the object to CPU memory.\n",
      "        numpy(): Convert the object to a numpy array.\n",
      "        cuda(): Move the object to CUDA memory.\n",
      "        to(*args, **kwargs): Move the object to the specified device.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(results[0].boxes.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8511], device='cuda:0')\n",
      "data: tensor([[ 38.5514, 323.2926,  90.5506, 413.6427,   0.8511,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (500, 1020)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.5510, 368.4677,  51.9992,  90.3500]], device='cuda:0')\n",
      "xywhn: tensor([[0.0633, 0.7369, 0.0510, 0.1807]], device='cuda:0')\n",
      "xyxy: tensor([[ 38.5514, 323.2926,  90.5506, 413.6427]], device='cuda:0')\n",
      "xyxyn: tensor([[0.0378, 0.6466, 0.0888, 0.8273]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(results[0].boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
